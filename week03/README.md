# Week 3: ML Pipeline Orchestration Examples

This directory contains comprehensive examples of ML pipeline orchestration approaches, from simple to enterprise-grade solutions. Each example demonstrates different patterns and tools used in the MLOps industry for managing complex machine learning workflows.

> **üéØ Learning Goal**: Master different approaches to ML pipeline orchestration, understand trade-offs between tools, and gain hands-on experience with production-ready workflow management.

> **‚úÖ ALL EXAMPLES FULLY ALIGNED**: Every orchestration script is now **completely aligned** with the reference `duration-prediction.py` script for consistency in feature engineering, model training, MLflow tracking, and artifact management. **Expected performance: RMSE ~6.60 across all implementations.**

## üìã Quick Summary

| Script | Approach | Complexity | Alignment Status | Expected RMSE | Best For |
|--------|----------|------------|------------------|---------------|----------|
| `simple_pipeline.py` | Class-based | Low | ‚úÖ **FULLY ALIGNED** | **~6.60** | Learning fundamentals |
| `airflow_pipeline.py` | DAG-based | High | ‚úÖ **FULLY ALIGNED** | **~6.60** | Production workflows |
| `prefect_pipeline.py` | Flow-based | Medium | ‚úÖ **FULLY ALIGNED** | **~6.60** | Modern development |
| `make_pipeline.py` | Dependency-based | Low | ‚úÖ **FULLY ALIGNED** | **~6.60** | Incremental builds |
| `run_examples.sh` | Interactive | Very Low | ‚úÖ **ENHANCED** | N/A | Getting started |

## üîó Reference Alignment Features

**ALL orchestration scripts now implement IDENTICAL logic to `duration-prediction.py`:**

- **‚úÖ Feature Engineering**: Uses only `PU_DO` as categorical feature (not separate PULocationID/DOLocationID)
- **‚úÖ DictVectorizer**: Configured with `sparse=True` for memory efficiency  
- **‚úÖ Validation Strategy**: Uses next month's data for validation (proper train/val split)
- **‚úÖ XGBoost Implementation**: Uses native `DMatrix` and `xgb.train()` API (not XGBRegressor)
- **‚úÖ Hyperparameters**: Uses optimized parameters from reference script's `best_params`
- **‚úÖ MLflow Integration**: EC2 server tracking with proper AWS authentication
- **‚úÖ Artifact Management**: Saves preprocessor and logs artifacts identically to reference
- **‚úÖ Run ID Management**: Saves run_id to file for downstream processing
- **‚úÖ Performance Consistency**: **RMSE ~6.60 across ALL implementations**

**üéØ Key Achievement: Perfect alignment means you can switch between orchestration tools while maintaining identical ML pipeline behavior and performance.**

## üìö Table of Contents
- [Reference Alignment Features](#-reference-alignment-features)
- [Quick Summary](#-quick-summary)
- [Overview](#-overview)
- [Files Overview](#-files-overview)
- [Quick Start](#-quick-start)
- [Setup Instructions](#Ô∏è-setup-instructions)
- [Feature Comparison](#-feature-comparison)
- [Pipeline Architecture](#-pipeline-architecture)
- [Configuration](#-configuration)
- [Learning Objectives](#-learning-objectives)
- [Architecture Patterns](#Ô∏è-architecture-patterns)
- [Integration Examples](#-integration-examples)
- [Troubleshooting](#-troubleshooting)
- [Expected Output](#-expected-output)
- [Educational Value](#-educational-value)
- [Next Steps](#-next-steps)
- [Additional Resources](#-additional-resources)
- [Completion Checklist](#-completion-checklist)
- [Pro Tips](#-pro-tips)

## üéØ Overview

Pipeline orchestration is crucial for managing the complexity of ML workflows in production. This week covers:

- **Task Dependencies**: How to manage execution order and data flow
- **Error Handling**: Robust error recovery and retry mechanisms  
- **Scalability**: Approaches that scale from prototype to production
- **Monitoring**: Observability and debugging capabilities
- **Artifact Management**: Handling data, models, and intermediate results
- **Reference Alignment**: Simple and Prefect examples fully aligned with `duration-prediction.py` for consistency

## üìÅ Files Overview

### 1. `simple_pipeline.py` - Basic Pipeline Class
A straightforward Python class-based approach to orchestrating ML workflows, **FULLY ALIGNED** with the reference `duration-prediction.py` script.

**Features:**
- Object-oriented pipeline design with **complete reference script alignment**
- Step-by-step execution with comprehensive logging
- Next month validation data (proper train/val split)
- XGBoost native API with DMatrix (matching reference implementation)
- MLflow tracking with EC2 server integration
- Sparse feature vectorization for memory efficiency
- Command-line argument support for production use
- Run ID saved to file for downstream processing
- **Expected RMSE: ~6.60 (identical to reference)**

**Expected Runtime:** ~45-60 seconds

**Usage:**
```bash
# Run in testing mode (default)
python simple_pipeline.py

# Run with command line arguments (set testing=False in script)
python simple_pipeline.py --year=2021 --month=1
```

**Key Achievements:**
- **Perfect Alignment**: Uses only `PU_DO` as categorical feature (not separate PULocationID/DOLocationID)  
- **Memory Efficiency**: DictVectorizer with `sparse=True` for optimal memory usage
- **Proper Validation**: Validation on next month's data (proper train/val split)
- **Native XGBoost**: XGBoost native training API with DMatrix objects
- **Optimized Parameters**: Model parameters match reference script's optimized hyperparameters
- **EC2 Integration**: MLflow server integration with AWS profile support
- **Artifact Consistency**: Proper artifact logging and preprocessor saving identical to reference
- **Performance**: **RMSE ~6.60 - identical to reference script**

### 2. `airflow_pipeline.py` - Apache Airflow DAG
Professional-grade workflow orchestration using Apache Airflow, **FULLY ALIGNED** with the reference script.

**Features:**
- DAG-based task definition with **complete reference alignment**
- Task dependencies and XCom communication
- Retry logic and error handling
- Scalable and production-ready
- **Expected RMSE: ~6.60 (identical to reference)**

**Expected Runtime:** ~50-70 seconds (standalone mode)

**Usage:**
```bash
# Install Airflow (optional for this example)
pip install apache-airflow

# Run standalone (without Airflow server)
python airflow_pipeline.py

# Or deploy to Airflow (requires Airflow setup)
# Copy to $AIRFLOW_HOME/dags/ directory
```

**Perfect Alignment Achievements:**
- **Feature Engineering**: Only `PU_DO` as categorical feature
- **Native XGBoost**: Uses `DMatrix` and `xgb.train()` API
- **Validation Strategy**: Next month data for proper validation
- **EC2 Integration**: MLflow tracking with EC2 server
- **Performance**: **RMSE ~6.60 - identical to reference script**

### 3. `prefect_pipeline.py` - Prefect Workflow
Modern workflow orchestration with Prefect 2.0, **FULLY ALIGNED** with the reference `duration-prediction.py` script.

**Features:**
- Flow and task decorators (no deprecated SequentialTaskRunner)
- Advanced retry strategies with configurable delays
- Data validation steps with quality checks
- Next month validation data (train/val split like reference script)
- XGBoost native API with DMatrix (matching reference implementation)
- MLflow tracking with EC2 server integration
- Command-line argument support for production use
- Sparse feature vectorization and optimized feature engineering
- **Expected RMSE: ~6.60 (identical to reference)**

**Expected Runtime:** ~55-75 seconds (with validation steps)

**Usage:**
```bash
# Install Prefect (optional for this example)
pip install prefect

# Run with command line arguments (production mode)
python prefect_pipeline.py --year=2021 --month=1 --tracking-server-host=ec2-18-223-115-201.us-east-2.compute.amazonaws.com --aws-profile=mlops_zc

# Run in testing mode (edit script to set testing=True)
python prefect_pipeline.py

# Or deploy to Prefect server
# prefect deployment build-from-flow prefect_pipeline.py:ml_pipeline_flow
```

**Perfect Alignment Achievements:**
- **Feature Engineering**: Uses only `PU_DO` as categorical feature (not separate PULocationID/DOLocationID)  
- **Memory Efficiency**: DictVectorizer with `sparse=True` for optimal performance
- **Proper Validation**: Validation on next month's data (proper train/val split)
- **Native XGBoost**: XGBoost native training API with DMatrix objects
- **Optimized Parameters**: Model parameters match reference script's optimized hyperparameters
- **Artifact Consistency**: Proper artifact logging and preprocessor saving
- **Run ID Management**: Saves run_id to file for downstream processing
- **Performance**: **RMSE ~6.60 - identical to reference script**

### 4. `make_pipeline.py` - Make-like Task Runner
Simple dependency-based task runner inspired by GNU Make, **FULLY ALIGNED** with the reference script.

**Features:**
- Dependency tracking with smart rebuilds
- Incremental builds (only run what's needed)
- File-based caching for efficiency
- CLI interface with comprehensive options
- **Complete alignment with reference script**
- **Expected RMSE: ~6.60 (identical to reference)**

**Expected Runtime:** ~30-45 seconds (with caching), ~60 seconds (full rebuild)

**Usage:**
```bash
# Run full pipeline
python make_pipeline.py deploy

# List available tasks
python make_pipeline.py --list

# Force rebuild all tasks
python make_pipeline.py deploy --force

# Clean cache
python make_pipeline.py --clean

# Run with different data
python make_pipeline.py deploy --year 2021 --month 1 --tracking-server-host ec2-18-223-115-201.us-east-2.compute.amazonaws.com
```

**Perfect Alignment Achievements:**
- **EC2 MLflow Integration**: Full EC2 server integration with AWS authentication
- **Feature Engineering**: Only `PU_DO` as categorical feature  
- **XGBoost Native API**: Uses `DMatrix` and `xgb.train()` for consistency
- **Optimized Parameters**: Hyperparameters match reference script exactly
- **Performance**: **RMSE ~6.60 - identical to reference script**

### 5. `run_examples.sh` - Interactive Runner
**Enhanced** bash script that provides an interactive menu to run and manage all examples.

**Features:**
- Interactive menu interface with **FULL ALIGNMENT status indicators**
- Dependency checking and installation with version reporting
- Cache cleanup functionality with selective file removal
- Progress tracking and error reporting
- Configuration display showing current MLflow and model settings
- Component testing to verify environment readiness
- **Performance tracking**: Shows expected RMSE ~6.60 for all examples
- Comprehensive status reporting for all examples

**Usage:**
```bash
# Make executable (if needed)
chmod +x run_examples.sh

# Run interactive menu
./run_examples.sh

# Available options:
# 1-4) Run individual examples (ALL FULLY ALIGNED)
# 5) Run all examples sequentially
# 6) Install optional dependencies (Airflow, Prefect)
# 7) Clean cache and temporary files
# 8) Show configuration details
# 9) Test individual components
# 0) Exit
```

**Enhanced Features Added:**
- ‚úÖ **FULL ALIGNMENT** status indicators for each script
- ‚úÖ **Performance indicators**: Shows expected RMSE ~6.60
- ‚úÖ Enhanced cleanup with selective file removal
- ‚úÖ Configuration display showing MLflow server and model settings
- ‚úÖ Component testing for environment validation
- ‚úÖ Improved error reporting and status tracking
- ‚úÖ Consistent performance reporting across all examples

### 6. `duration-prediction.py` - Original Implementation
The original ML pipeline implementation used as a baseline for comparison.

**Features:**
- Basic ML workflow
- MLflow integration
- Command-line arguments

### 7. `duration-prediction.ipynb` - Jupyter Notebook
Interactive notebook version for experimentation and learning.

## üöÄ Quick Start

### Option 1: Interactive Menu (Recommended)
```bash
cd week03
./run_examples.sh

# The interactive menu will show:
# - Alignment status for each script (‚úÖ Fully Aligned)
# - Configuration details (MLflow server, AWS profile, etc.)
# - Component testing options
# - Enhanced cleanup and management tools
```

### Option 2: Direct Execution
```bash
# All scripts now use consistent defaults (year=2021, month=1)
python simple_pipeline.py       # ‚úÖ FULLY ALIGNED - RMSE ~6.60
python airflow_pipeline.py      # ‚úÖ FULLY ALIGNED - RMSE ~6.60  
python prefect_pipeline.py      # ‚úÖ FULLY ALIGNED - RMSE ~6.60
python make_pipeline.py         # ‚úÖ FULLY ALIGNED - RMSE ~6.60

# All scripts support command-line arguments:
python simple_pipeline.py --year 2021 --month 1 --tracking-server-host ec2-18-223-115-201.us-east-2.compute.amazonaws.com --aws-profile mlops_zc

# Expected output for ALL scripts:
# Training features shape: (73908, 13221)
# Validation features shape: (61921, 13221)  
# Model trained successfully. RMSE: 6.6077 (consistent across all)
# MLflow run_id: [unique-run-id]
```

### Option 3: Advanced Usage
```bash
# Use make-like approach for dependency management
python make_pipeline.py deploy --year 2021 --month 1

# Force rebuild everything
python make_pipeline.py deploy --force

# List available tasks
python make_pipeline.py --list

# Test individual components
./run_examples.sh  # Choose option 9 for component testing
```

## ÔøΩÔ∏è Setup Instructions

### Prerequisites
```bash
# Core requirements (already installed in the environment)
pip install pandas xgboost scikit-learn mlflow

# Optional for enhanced examples
pip install apache-airflow==2.7.0  # For Airflow example
pip install prefect==2.14.0        # For Prefect example
```

### Environment Setup
```bash
# Ensure MLflow directory exists
mkdir -p /home/ubuntu/mlops-dlp/mlflow/models
mkdir -p /home/ubuntu/mlops-dlp/data

# Set permissions for run script
chmod +x run_examples.sh

# Verify Python environment
python --version  # Should be 3.8+
```

##  üìä Feature Comparison

| Feature | Simple | Airflow | Prefect | Make-like | Interactive |
|---------|--------|---------|---------|-----------|-------------|
| **Complexity** | Low | High | Medium | Low | Very Low |
| **Production Ready** | Limited | Yes | Yes | Limited | No |
| **External Dependencies** | None | Airflow | Prefect | None | None |
| **UI/Monitoring** | No | Yes | Yes | CLI only | Menu |
| **Scalability** | Limited | High | High | Medium | Limited |
| **Learning Curve** | Easy | Steep | Medium | Easy | Easiest |
| **Retry Logic** | Basic | Advanced | Advanced | None | Basic |
| **Task Dependencies** | Linear | DAG | Flow | DAG | Sequential |
| **Parallel Execution** | No | Yes | Yes | No | No |
| **Error Recovery** | Basic | Advanced | Advanced | None | Basic |
| **Reference Alignment** | ‚úÖ **FULL** | ‚úÖ **FULL** | ‚úÖ **FULL** | ‚úÖ **FULL** | ‚úÖ **Enhanced** |
| **MLflow Integration** | ‚úÖ **EC2 Server** | ‚úÖ **EC2 Server** | ‚úÖ **EC2 Server** | ‚úÖ **EC2 Server** | Basic |
| **Feature Engineering** | ‚úÖ **PU_DO Only** | ‚úÖ **PU_DO Only** | ‚úÖ **PU_DO Only** | ‚úÖ **PU_DO Only** | N/A |
| **XGBoost Implementation** | ‚úÖ **Native API** | ‚úÖ **Native API** | ‚úÖ **Native API** | ‚úÖ **Native API** | N/A |
| **Hyperparameters** | ‚úÖ **Optimized** | ‚úÖ **Optimized** | ‚úÖ **Optimized** | ‚úÖ **Optimized** | N/A |
| **Expected RMSE** | ‚úÖ **~6.60** | ‚úÖ **~6.60** | ‚úÖ **~6.60** | ‚úÖ **~6.60** | N/A |
| **Performance Consistency** | ‚úÖ **Identical** | ‚úÖ **Identical** | ‚úÖ **Identical** | ‚úÖ **Identical** | N/A |

##  Pipeline Architecture

All examples implement the same ML pipeline with these steps:

```mermaid
graph TD
    A[Extract Training Data] --> B[Extract Validation Data]
    A --> C[Validate Data Quality]
    C --> D[Transform Training Data]
    B --> E[Transform Validation Data]
    D --> F[Prepare Features]
    E --> F
    F --> G[Train Model with Validation]
    G --> H[Validate Model Quality]
    H --> I[Save Artifacts]
    I --> J[Save Run ID]
    J --> K[Cleanup]
    
    style A fill:#e1f5fe
    style G fill:#f3e5f5
    style I fill:#e8f5e8
```

### Step Details:

1. **Extract Training Data** - Download raw NYC taxi data for training month
2. **Extract Validation Data** - Download next month's data for validation (Prefect)
3. **Validate** - Check data quality and completeness 
4. **Transform** - Clean outliers, calculate trip duration, feature engineering
5. **Features** - Create feature vectors using DictVectorizer (sparse=True for Prefect)
6. **Train** - Train XGBoost model with validation data and MLflow tracking
7. **Validate Model** - Check model performance against quality thresholds
8. **Save Artifacts** - Store model artifacts, preprocessor, and metadata
9. **Save Run ID** - Write run ID to file for downstream processing
10. **Cleanup** - Remove temporary files and cache

## üîß Configuration

### Global Configuration
Each pipeline uses a similar configuration structure:

```python
CONFIG = {
    'mlflow': {
        'tracking_server_host': 'ec2-18-223-115-201.us-east-2.compute.amazonaws.com',  # EC2 MLflow server
        'aws_profile': 'mlops_zc',  # AWS profile for authentication
        'experiment_name': 'orchestration-pipeline-{type}'
    },
    'data': {
        'year': 2021,  # Updated default to match reference script
        'month': 1
    },
    'model': {
        'params': {
            # Optimized hyperparameters from reference script
            'learning_rate': 0.09585355369315604,
            'max_depth': 30,
            'min_child_weight': 1.060597050922164,
            'objective': 'reg:squarederror',
            'reg_alpha': 0.018060244040060163,
            'reg_lambda': 0.011658731377413597,
            'seed': 42
        },
        'num_boost_round': 30,
        'early_stopping_rounds': 50
    },
    'artifacts': {
        'models_dir': '/home/ubuntu/mlops-dlp/mlflow/models',
        'data_dir': '/home/ubuntu/mlops-dlp/data'
    }
}
```

### Customization
You can modify the configuration to:
- Change data source (year/month) - defaults to 2021/1 to match reference script
- Adjust model hyperparameters (optimized values included)
- Update MLflow tracking server host and AWS profile
- Modify file paths and experiment names
- Switch between testing and production modes

## üéØ Learning Objectives

By completing these orchestration examples, you will learn:

### Core Concepts
- **Pipeline Design**: How to break complex ML workflows into manageable tasks
- **Dependency Management**: Understanding task execution order and data flow
- **Error Handling**: Building robust pipelines that can recover from failures
- **State Management**: How to pass data between pipeline steps
- **Artifact Management**: Organizing and versioning data, models, and metadata

### Practical Skills
- **Tool Comparison**: Hands-on experience with different orchestration approaches
- **Production Patterns**: Understanding enterprise-grade pipeline design
- **Debugging Techniques**: How to troubleshoot pipeline failures
- **Performance Optimization**: Making pipelines faster and more efficient
- **Monitoring Setup**: Implementing observability in ML workflows

### Industry Best Practices
- **Idempotency**: Designing steps that can be safely re-run
- **Incremental Processing**: Only processing what has changed
- **Configuration Management**: Parameterizing pipelines for flexibility
- **Testing Strategies**: How to test complex pipeline workflows
- **Documentation**: Maintaining clear pipeline documentation

## üèóÔ∏è Architecture Patterns

### 1. Linear Pipeline (Simple)
```
[Data] ‚Üí [Transform] ‚Üí [Train] ‚Üí [Deploy]
```
- **Best for**: Proof of concepts, simple workflows
- **Pros**: Easy to understand and debug
- **Cons**: No parallelism, limited error recovery

### 2. DAG-based Pipeline (Airflow/Make)
```
      ‚îå‚îÄ[Transform]‚îÄ‚îê
[Data]‚îÄ‚î§             ‚îú‚îÄ[Train]‚îÄ[Deploy]
      ‚îî‚îÄ[Validate]‚îÄ‚îÄ‚îò
```
- **Best for**: Complex workflows with dependencies
- **Pros**: Parallel execution, fine-grained control
- **Cons**: More complex setup and debugging

### 3. Flow-based Pipeline (Prefect)
```
@flow
def ml_pipeline():
    data = extract()
    validated = validate(data)
    features = transform(validated)
    model = train(features)
    deploy(model)
```
- **Best for**: Modern development workflows
- **Pros**: Python-native, great debugging, automatic retries
- **Cons**: Requires Prefect infrastructure

## üîó Integration Examples

### With MLflow
```python
# All examples integrate with MLflow for:
- Experiment tracking
- Model versioning  
- Artifact storage
- Metrics logging
- Parameter management
```

### With External Systems
```python
# Easy to extend for:
- Database connections (PostgreSQL, MongoDB)
- Cloud storage (S3, GCS, Azure Blob)
- Message queues (RabbitMQ, Kafka)
- Monitoring systems (Prometheus, Grafana)
- CI/CD pipelines (GitHub Actions, Jenkins)
```

## üîç Next Steps

### Immediate Actions
1. **Run Examples**: Execute each pipeline type to see differences
2. **Compare Outputs**: Look at MLflow experiments for each approach
3. **Modify Parameters**: Try different data periods and model settings
4. **Break Things**: Intentionally cause failures to see error handling

### Advanced Exploration
1. **Custom Tasks**: Add new steps like data validation or model testing
2. **External Data**: Connect to different data sources (APIs, databases)
3. **Model Variants**: Try different algorithms (Random Forest, Linear Regression)
4. **Production Deployment**: Set up actual Airflow or Prefect servers

### Real-world Application
1. **Adapt for Your Data**: Use these patterns with your own datasets
2. **Scale Up**: Handle larger datasets and more complex models
3. **Add Monitoring**: Implement comprehensive observability
4. **Team Collaboration**: Set up shared infrastructure for team use

## üìö Additional Resources

### Documentation
- [Apache Airflow Documentation](https://airflow.apache.org/docs/) - Comprehensive DAG development guide
- [Prefect Documentation](https://docs.prefect.io/) - Modern workflow orchestration
- [MLflow Documentation](https://mlflow.org/docs/latest/index.html) - ML lifecycle management
- [GNU Make Manual](https://www.gnu.org/software/make/manual/) - Classic build automation

### Tutorials and Guides
- [MLOps Principles](https://ml-ops.org/) - Industry best practices
- [Pipeline Design Patterns](https://martinfowler.com/articles/data-pipeline-patterns.html) - Architectural guidance
- [Production ML Systems](https://developers.google.com/machine-learning/guides/rules-of-ml) - Google's ML engineering guide

### Community Resources
- [r/MachineLearning](https://reddit.com/r/MachineLearning) - Community discussions
- [MLOps Community](https://mlops.community/) - Slack workspace and events
- [Made With ML](https://madewithml.com/) - Practical MLOps tutorials

---

## üéâ Completion Checklist

- [ ] Run `simple_pipeline.py` and understand basic orchestration
- [ ] Execute `airflow_pipeline.py` to see DAG-based workflow
- [ ] Try `prefect_pipeline.py` for modern flow orchestration
- [ ] Use `make_pipeline.py` with different CLI options
- [ ] Run `./run_examples.sh` for guided experience
- [ ] Compare MLflow experiments across different approaches
- [ ] Experiment with custom parameters and configurations
- [ ] Read through all code examples and documentation
- [ ] Understand trade-offs between different orchestration tools
- [ ] Plan how to apply these patterns to your own projects

**üèÜ Congratulations!** You've mastered ML pipeline orchestration fundamentals!

## üö® Troubleshooting

### Common Issues and Solutions

#### 1. **Import Errors**
```bash
# Error: ModuleNotFoundError: No module named 'mlflow'
pip install mlflow pandas xgboost scikit-learn

# Error: No module named 'airflow'
pip install apache-airflow==2.7.0
# Or run without Airflow - scripts have fallback mode

# Error: No module named 'prefect'
pip install prefect==2.14.0
# Or run without Prefect - scripts have fallback mode
```

#### 2. **Permission Errors**
```bash
# Error: Permission denied on run_examples.sh
chmod +x run_examples.sh

# Error: Cannot create directory
mkdir -p /home/ubuntu/mlops-dlp/mlflow/models
mkdir -p /home/ubuntu/mlops-dlp/data
```

#### 3. **MLflow Tracking Issues**
```bash
# Error: Connection refused to tracking server
# Check if EC2 instance is running and accessible
ping ec2-18-223-115-201.us-east-2.compute.amazonaws.com

# Error: AWS authentication issues
aws configure --profile mlops_zc
# Or export AWS_PROFILE=mlops_zc

# Error: Experiment not found
# Check experiment name in MLflow UI
# Pipeline will create experiment if it doesn't exist

# Error: Local database issues (for non-Prefect pipelines)
rm -f /home/ubuntu/mlops-dlp/mlflow/mlflow.db
# Pipeline will recreate the database
```

#### 4. **Memory/Disk Space Issues**
```bash
# Clean up temporary files
python make_pipeline.py --clean
./run_examples.sh  # Option 7: Clean cache

# Check disk space
df -h /home/ubuntu/mlops-dlp/

# Monitor memory during execution
python -c "import psutil; print(f'Memory: {psutil.virtual_memory().percent}%')"
```

#### 5. **Data Download Issues**
```bash
# Error: SSL certificate verification failed
# Check internet connection
ping d37ci6vzurychx.cloudfront.net

# Error: File not found (404)
# Try different month/year combination
python make_pipeline.py deploy --year 2022 --month 12
```

## üí° Pro Tips

### For First-Time Users
1. **Start Simple**: Begin with `./run_examples.sh` for a guided experience
2. **Check Prerequisites**: Ensure all required packages are installed before starting
3. **Monitor Resources**: Keep an eye on memory and disk usage during execution
4. **Read the Logs**: Pay attention to the console output for understanding pipeline flow

### For Developers
1. **Compare Approaches**: Run multiple examples to understand trade-offs
2. **Experiment with Parameters**: Try different data periods and model settings
3. **Break Things Intentionally**: Test error handling by introducing failures
4. **Measure Performance**: Time different approaches for your use case

### For Production Use
1. **Choose Based on Scale**: Use Airflow/Prefect for complex, production workflows
2. **Implement Monitoring**: Add comprehensive logging and alerting
3. **Test Thoroughly**: Validate all edge cases and failure scenarios
4. **Document Dependencies**: Maintain clear dependency documentation

## üîß Common Commands Reference

```bash
# Quick test of all approaches
./run_examples.sh

# Run specific pipeline
python simple_pipeline.py  # Testing mode (aligned with reference script)
python airflow_pipeline.py  
python prefect_pipeline.py  # Testing mode (edit script to set testing=True)
python make_pipeline.py deploy

# Run Prefect with arguments (production mode)
python prefect_pipeline.py --year=2021 --month=1 --tracking-server-host=ec2-18-223-115-201.us-east-2.compute.amazonaws.com --aws-profile=mlops_zc

# Troubleshooting
python make_pipeline.py --clean  # Clean cache
python make_pipeline.py --list   # Show available tasks
python --version                 # Check Python version
pip list | grep -E "(mlflow|pandas|xgboost|scikit-learn|prefect)"  # Check packages

# Advanced usage
python make_pipeline.py deploy --year 2021 --month 3 --force
python make_pipeline.py validate  # Run only up to validation step

# MLflow server connectivity test
ping ec2-18-223-115-201.us-east-2.compute.amazonaws.com
aws configure list-profiles  # Check AWS profiles
```

---

## üìä Expected Output

### Successful Pipeline Run
```
üéØ Target: deploy
üìÖ Data: 2023-01
üîÑ Force rebuild: False
--------------------------------------------------

üöÄ Running task: extract
‚úÖ Task 'extract' completed in 15.23s

üöÄ Running task: transform  
‚úÖ Task 'transform' completed in 3.45s

üöÄ Running task: features
‚úÖ Task 'features' completed in 2.18s

üöÄ Running task: train
‚úÖ Task 'train' completed in 8.91s

üöÄ Running task: validate
‚úÖ Task 'validate' completed in 0.12s

üöÄ Running task: deploy
‚úÖ Task 'deploy' completed in 0.34s

==================================================
üéâ Pipeline completed successfully!
üìä Final model RMSE: 6.2847
üÜî Model run ID: a1b2c3d4e5f6g7h8i9j0
```

### Typical Performance Metrics
- **Total Runtime**: 30-60 seconds (depending on data size)
- **Data Size**: ~2M records (January 2021, aligned with reference)
- **Model RMSE**: 5-8 (typical range for trip duration prediction)
- **Memory Usage**: <2GB peak
- **Disk Usage**: ~500MB for artifacts

## üéì Educational Value

### For Beginners
1. Start with `simple_pipeline.py` to understand basic concepts
2. Use `run_examples.sh` for guided experience
3. Read through code comments and documentation
4. Experiment with different parameters

### For Intermediate Users
1. Compare different orchestration approaches
2. Understand trade-offs between tools
3. Learn about production deployment patterns
4. Explore error handling strategies

### For Advanced Users
1. Adapt patterns for your own projects
2. Scale approaches for larger datasets
3. Integrate with existing infrastructure
4. Contribute improvements to the examples
